\documentclass[a4paper,12pt]{report}
\usepackage[dvipsnames]{xcolor}
\usepackage{subcaption}
\definecolor{mauChinh}{RGB}{0, 80, 180}
\definecolor{mauLienKet}{RGB}{0, 0, 200}
\definecolor{mauDoDam}{RGB}{192, 0, 0}
\definecolor{codeBackground}{RGB}{240, 240, 240}
\usepackage[vietnamese]{babel}
\usepackage[normalem]{ulem}
\usepackage[a4paper,left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{unicode-math}
\setmainfont{Libertinus Serif}
\setmathfont{Libertinus Math}
\usepackage{graphicx}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, calc, angles, quotes}
\usepackage{tabularx,booktabs,makecell,array, longtable}
\usepackage{multirow}
\renewcommand{\arraystretch}{1.5}
\usepackage[labelsep=period, font=small, labelfont=bf]{caption}
\usepackage{float}
\usepackage[most]{tcolorbox}
\usepackage{fancyhdr}
\usepackage{tocloft}
\usepackage{setspace}
\setstretch{1.5}
\usepackage{ragged2e}
\usepackage{listings}
\usepackage{xcolor}

% Cấu hình listings cho code Python
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    backgroundcolor=\color{codeBackground},
    breaklines=true,
    breakatwhitespace=true,
    frame=single,
    rulecolor=\color{gray!30},
    numbers=left,
    numbersep=5pt,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue!100}\bfseries,
    commentstyle=\color{gray!70}\itshape,
    stringstyle=\color{red!100},
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    captionpos=b,
    tabsize=4,
    columns=fixed,
    keepspaces=true,
    aboveskip=10pt,
    belowskip=10pt
}

\let\oldsum\sum
\renewcommand{\sum}{\displaystyle\oldsum}
\let\oldprod\prod
\renewcommand{\prod}{\displaystyle\oldprod}
\let\oldlim\lim
\renewcommand{\lim}{\displaystyle\oldlim}
\let\oldint\int
\renewcommand{\int}{\displaystyle\oldint}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE]{\nouppercase{\leftmark}}
\fancyhead[RO]{\nouppercase{\rightmark}}
\fancyfoot[CE,CO]{\thepage}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0pt}
\fancypagestyle{plain}{
	\fancyhf{}
	\fancyfoot[CE,CO]{\thepage}
	\renewcommand{\headrulewidth}{0pt}
}

\renewcommand{\cftchapfont}{\bfseries}
\renewcommand{\cftsecfont}{\bfseries}
\setlength{\cftchapnumwidth}{2.5em}
\setlength{\cftsecnumwidth}{2.5em}

\usepackage[
plainpages=false,
colorlinks=true,
linkcolor=mauLienKet,
urlcolor=mauLienKet,
citecolor=mauDoDam,
pdftitle={Dự đoán Điểm Toán học của Học sinh - Phân tích Dữ liệu Giáo dục},
pdfauthor={Bùi Quang Chiến},
pdfsubject={Dự đoán Kết quả Học tập sử dụng XGBoost},
pdfkeywords={Machine Learning, XGBoost, Regression, Dự đoán Điểm số, Khai phá Dữ liệu Giáo dục},
pdfcreator={XeLaTeX with hyperref}
]{hyperref}

\usepackage[
backend=biber,
style=numeric,
sorting=none,
maxbibnames=99
]{biblatex}
\addbibresource{tailieu.bib}

\begin{document}
	
	%===========================================================
	% TRANG BÌA
	%===========================================================
	\include{Sections/1-Title}
	
	\newpage
	\thispagestyle{plain}
	\tableofcontents
	\newpage
	
	%===========================================================
	% TÓM TẮT
	%===========================================================
	\chapter*{Tóm tắt}
	\addcontentsline{toc}{chapter}{Tóm tắt}
	
	Báo cáo này trình bày một nghiên cứu về dự đoán kết quả học tập học sinh, sử dụng bộ dữ liệu ``Student Math Performance'' với 395 quan sát và 30 đặc trưng độc lập (không bao gồm các biến điểm số G1, G2, G3), bao gồm các thông tin nhân khẩu học, kinh tế-xã hội và hành vi học tập. Chúng tôi áp dụng các mô hình học máy, đặc biệt là XGBoost Regression, để xây dựng các bộ dự báo nhằm xác định các yếu tố ảnh hưởng đến thành tích học sinh.
	
	Kết quả cho thấy mô hình XGBoost đạt hiệu suất tối ưu với $R^2 = 0.26$, RMSE = 12.26 và MAE = 9.87, vượt trội so với mô hình Linear Regression cơ sở. Phân tích độ quan trọng đặc trưng (Feature Importance) chỉ ra rằng các yếu tố như tình trạng gia đình, môi trường học tập và sự hỗ trợ giáo dục là những yếu tố dự báo mạnh mẽ nhất. Nghiên cứu cung cấp các thông tin chi tiết để hỗ trợ quyết định chính sách giáo dục và can thiệp sớm nhằm cải thiện thành tích học sinh.
	
	\textbf{Từ khóa:} Dự đoán Kết quả Học tập, XGBoost, Khai phá Dữ liệu Giáo dục, Hồi quy
	
	\newpage
	
	%===========================================================
	% CHƯƠNG 1: GIỚI THIỆU
	%===========================================================
	\chapter{Giới thiệu}
	
	\section{Bối cảnh và Ý nghĩa}
	
	Dự đoán kết quả học tập là một bài toán quan trọng trong lĩnh vực khai phá dữ liệu giáo dục (Educational Data Mining - EDM). Việc xác định sớm những sinh viên gặp khó khăn về học tập cho phép các cơ sở giáo dục tư vấn, hỗ trợ và can thiệp kịp thời, từ đó nâng cao chất lượng giáo dục và giảm tỷ lệ bỏ học.
	
	Các yếu tố ảnh hưởng đến thành tích học sinh rất đa dạng: từ đặc điểm cá nhân (giới tính, tuổi), điều kiện gia đình (trình độ học vấn phụ huynh, tình trạng kinh tế), cho đến hành vi học tập (thời gian học, tham gia hoạt động). Mô hình học máy có khả năng khám phá các mối quan hệ phức tạp giữa các yếu tố này và kết quả học tập, từ đó cung cấp cái nhìn sâu sắc cho các nhà hoạch định chính sách.
	
	\section{Mục tiêu nghiên cứu}
	
	Mục tiêu chính của dự án này là:
	
	\begin{enumerate}
		\item \textbf{Xây dựng mô hình dự báo:} Phát triển các mô hình học máy (Linear Regression, XGBoost) để dự đoán điểm toán học (G3) dựa trên 30 đặc trưng độc lập.
		
		\item \textbf{So sánh hiệu suất mô hình:} Đánh giá và so sánh hiệu suất của các mô hình sử dụng các chỉ số như $R^2$, RMSE và MAE.
		
		\item \textbf{Phân tích yếu tố quan trọng:} Xác định những yếu tố nào có ảnh hưởng lớn nhất đến kết quả học tập thông qua phân tích độ quan trọng đặc trưng.
		
		\item \textbf{Cung cấp hàm ý chính sách:} Đề xuất các biện pháp can thiệp dựa trên các phát hiện từ mô hình.
	\end{enumerate}
	
	\newpage
	
	%===========================================================
	% CHƯƠNG 2: DỮ LIỆU
	%===========================================================
	\chapter{Dữ liệu}
	
	\section{Nguồn dữ liệu}
	
	Dữ liệu được sử dụng trong nghiên cứu này là bộ dữ liệu ``Student Math Performance'' được thu thập từ hai trường phổ thông ở Bồ Đào Nha. Bộ dữ liệu gồm 395 quan sát (sinh viên) với 33 biến số, trong đó có 30 đặc trưng độc lập mô tả thông tin nhân khẩu học, kinh tế-xã hội, và hành vi học tập, cộng với 3 biến điểm số (G1, G2, G3). Biến mục tiêu là $G3$ (điểm toán học cuối năm, thang điểm 0-20).
	
	\section{Mô tả đặc trưng}
	
	Bộ dữ liệu bao gồm các loại đặc trưng sau:
	
	\subsection{Đặc trưng Nhân khẩu học}
	\begin{itemize}
		\item \texttt{school}: Trường học (GP - Gabriel Pereira, MS - Mousinho da Silveira)
		\item \texttt{sex}: Giới tính (F - Nữ, M - Nam)
		\item \texttt{age}: Tuổi (15-22)
		\item \texttt{address}: Nơi cư trú (U - Thành thị, R - Nông thôn)
	\end{itemize}
	
	\subsection{Đặc trưng Kinh tế-Xã hội Gia đình}
	\begin{itemize}
		\item \texttt{famsize}: Kích thước gia đình (LE3 - $\leq 3$ người, GT3 - $> 3$ người)
		\item \texttt{Pstatus}: Tình trạng bố mẹ (T - Sống chung, A - Ly thân)
		\item \texttt{Medu}: Trình độ học vấn của mẹ (0-4: từ không học đến giáo dục đại học)
		\item \texttt{Fedu}: Trình độ học vấn của bố (0-4: từ không học đến giáo dục đại học)
		\item \texttt{Mjob}: Nghề nghiệp của mẹ
		\item \texttt{Fjob}: Nghề nghiệp của bố
	\end{itemize}
	
	\subsection{Đặc trưng Hành vi Học tập}
	\begin{itemize}
		\item \texttt{traveltime}: Thời gian đi học (1-4: từ <15 phút đến >1 giờ)
		\item \texttt{studytime}: Thời gian học tập hàng tuần (1-4: từ <2 giờ đến >10 giờ)
		\item \texttt{failures}: Số lần trượt lớp trước đó
		\item \texttt{schoolsup}: Hỗ trợ giáo dục thêm (yes/no)
		\item \texttt{famsup}: Hỗ trợ giáo dục từ gia đình (yes/no)
		\item \texttt{paid}: Tham gia lớp trả phí thêm (yes/no)
		\item \texttt{activities}: Tham gia hoạt động ngoại khóa (yes/no)
		\item \texttt{nursery}: Có đi mẫu giáo (yes/no)
	\end{itemize}
	
	\subsection{Đặc trưng Cá nhân \& Xã hội}
	\begin{itemize}
		\item \texttt{reason}: Lý do chọn trường
		\item \texttt{guardian}: Người giám hộ chính
		\item \texttt{higher}: Có ý định học đại học (yes/no)
		\item \texttt{internet}: Có Internet ở nhà (yes/no)
		\item \texttt{romantic}: Có mối quan hệ tình cảm (yes/no)
		\item \texttt{famrel}: Chất lượng mối quan hệ gia đình (1-5)
		\item \texttt{freetime}: Thời gian rảnh rỗi (1-5)
		\item \texttt{goout}: Tần suất ra ngoài chơi (1-5)
		\item \texttt{Dalc}: Mức tiêu thụ rượu ngày thường (1-5)
		\item \texttt{Walc}: Mức tiêu thụ rượu cuối tuần (1-5)
		\item \texttt{health}: Tình trạng sức khỏe (1-5)
		\item \texttt{absences}: Số ngày vắng mặt
		\item \texttt{G1}, \texttt{G2}: Điểm kỳ 1 và kỳ 2 (0-20)
	\end{itemize}
	
	\subsection{Biến mục tiêu}
	\begin{itemize}
		\item \texttt{G3}: Điểm toán học cuối năm (0-20) - \textbf{Biến cần dự báo}
	\end{itemize}
	
	\section{Thống kê mô tả}
	
	Bộ dữ liệu bao gồm 395 sinh viên, không có giá trị thiếu. Biến mục tiêu $G3$ có trung bình 11.91 và độ lệch chuẩn 3.76. Một số đặc trưng là biến phân loại (categorical) và cần được mã hóa (encoding) trước khi đưa vào mô hình.
	
	\newpage
	
	%===========================================================
	% CHƯƠNG 3: PHƯƠNG PHÁP
	%===========================================================
	\chapter{Phương pháp}
	
	\section{Tiền xử lý dữ liệu}
	
	\subsection{Mã hóa các biến phân loại}
	
	Các biến danh mục (categorical) như \texttt{school}, \texttt{sex}, \texttt{address}, v.v. được mã hóa thành các biến nhị phân bằng phương pháp One-Hot Encoding:
	
	\begin{lstlisting}[caption={One-Hot Encoding}]
# One-Hot Encoding for categorical variables
X = pd.get_dummies(X, drop_first=True)
# This approach removes one column to avoid multicollinearity
	\end{lstlisting}
	
	\subsection{Chia tập dữ liệu}
	
	Dữ liệu được chia thành tập huấn luyện (80\%) và tập kiểm tra (20\%) với \texttt{random\_state=42} để đảm bảo tính lặp lại của kết quả:
	
	\begin{lstlisting}[caption={Train-Test Split}]
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, shuffle=True
)
	\end{lstlisting}
	
	\section{Các mô hình sử dụng}
	
	\subsection{Linear Regression}
	
	Linear Regression là mô hình cơ sở, giả định mối quan hệ tuyến tính giữa các đặc trưng và biến mục tiêu. Mô hình này được sử dụng để so sánh với các mô hình phức tạp hơn.
	
	\subsection{XGBoost Regression}
	
	XGBoost (eXtreme Gradient Boosting) là thuật toán boosting gradient nâng cao, xây dựng một tập hợp các cây quyết định thông qua quá trình lặp. Mỗi cây mới được huấn luyện để sửa chữa các lỗi (residuals) của cây trước đó, tạo thành một mô hình ensemble mạnh mẽ.
	
	\textbf{Nguyên lý hoạt động:}
	
	XGBoost tối ưu hóa hàm mục tiêu có dạng:
	$$\mathcal{L}(\phi) = \sum_i l(\hat{y}_i, y_i) + \sum_k \Omega(f_k)$$
	
	trong đó:
	\begin{itemize}
		\item $l(\hat{y}_i, y_i)$ là hàm mất mát (loss function) đo lường sai số dự đoán
		\item $\Omega(f_k)$ là số hạng regularization kiểm soát độ phức tạp mô hình
		\item $f_k$ là cây quyết định thứ $k$
	\end{itemize}
	
	\textbf{Ưu điểm của XGBoost cho bài toán dự đoán học tập:}
	\begin{itemize}
		\item \textbf{Xử lý phi tuyến:} Tự động phát hiện các tương tác phức tạp giữa các đặc trưng (ví dụ: tương tác giữa thời gian học và hỗ trợ gia đình)
		\item \textbf{Chống overfitting:} Sử dụng regularization (L1, L2) và early stopping
		\item \textbf{Xử lý dữ liệu thiếu:} Tự động học cách xử lý missing values
		\item \textbf{Feature importance:} Cung cấp ranking các đặc trưng quan trọng
		\item \textbf{Hiệu suất cao:} Tối ưu hóa tính toán song song và cache-aware
	\end{itemize}
	
	\textbf{Cấu hình mô hình XGBoost:}
	
	Các siêu tham số (hyperparameters) được tối ưu cho bài toán này:
	\begin{itemize}
		\item \texttt{objective='reg:squarederror'}: Hàm mất mát cho bài toán hồi quy (MSE)
		
		\item \texttt{n\_estimators=100}: Số lượng cây quyết định được xây dựng tuần tự. Giá trị 100 cân bằng giữa hiệu suất và thời gian huấn luyện. Quá ít cây → underfitting, quá nhiều → overfitting và tốn thời gian.
		
		\item \texttt{max\_depth=5}: Độ sâu tối đa của mỗi cây quyết định, kiểm soát độ phức tạp mô hình. 
		\begin{itemize}
			\item Độ sâu thấp (3-4): Mô hình đơn giản, tránh overfitting nhưng có thể bỏ lỡ tương tác phức tạp
			\item Độ sâu cao (7-10): Có thể học tương tác phức tạp nhưng dễ overfitting
			\item Giá trị 5: Cân bằng tốt giữa khả năng biểu diễn và tính tổng quát
		\end{itemize}
		
		\item \texttt{learning\_rate=0.1}: Tốc độ học (shrinkage factor), $\eta \in (0, 1]$. 
		\begin{itemize}
			\item Mỗi cây đóng góp chỉ $\eta$ lần trọng số vào dự đoán cuối cùng
			\item $\eta$ thấp (0.01-0.05): Học chậm nhưng tổng quát hóa tốt, cần nhiều cây hơn
			\item $\eta$ cao (0.3-0.5): Học nhanh nhưng dễ overfitting
			\item Giá trị 0.1: Tiêu chuẩn trong nhiều bài toán thực tế
		\end{itemize}
		
		\item \texttt{subsample=0.8}: Tỷ lệ mẫu con (row sampling) cho mỗi cây.
		\begin{itemize}
			\item Kỹ thuật Stochastic Gradient Boosting: Mỗi cây huấn luyện trên 80\% dữ liệu ngẫu nhiên
			\item Giảm phương sai (variance), tăng tính đa dạng giữa các cây
			\item Tránh overfitting bằng cách thêm nhiễu (noise) vào quá trình huấn luyện
		\end{itemize}
		
		\item \texttt{colsample\_bytree=0.8}: Tỷ lệ đặc trưng (column sampling) cho mỗi cây.
		\begin{itemize}
			\item Mỗi cây sử dụng ngẫu nhiên 80\% các đặc trưng
			\item Tăng tính decorrelation giữa các cây → ensemble tốt hơn
			\item Giảm ảnh hưởng của các đặc trưng chi phối (dominant features)
			\item Tương tự Random Forest nhưng áp dụng trong boosting
		\end{itemize}
		
		\item \texttt{random\_state=42}: Seed ngẫu nhiên cố định để đảm bảo tính lặp lại (reproducibility) - tiêu chuẩn trong nghiên cứu khoa học.
	\end{itemize}
	
	\textbf{Kỹ thuật Regularization:}
	
	XGBoost tích hợp các kỹ thuật regularization mạnh mẽ:
	\begin{itemize}
		\item \textbf{L1 regularization (Lasso):} Thưa hóa trọng số, loại bỏ đặc trưng không quan trọng
		\item \textbf{L2 regularization (Ridge):} Giảm độ lớn trọng số, tránh các hệ số quá lớn
		\item \textbf{Tree pruning:} Cắt tỉa cây dựa trên gain threshold
		\item \textbf{Shrinkage:} Learning rate làm giảm ảnh hưởng từng cây
		\item \textbf{Subsampling:} Row và column sampling tạo diversity
	\end{itemize}
	
	\begin{lstlisting}[caption={XGBoost Configuration}]
import xgboost as xgb

model = xgb.XGBRegressor(
    objective='reg:squarederror',
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1,
    verbosity=0
)
model.fit(X_train, y_train)
	\end{lstlisting}
	
	\section{Đánh giá mô hình}
	
	Các chỉ số hiệu suất sử dụng:
	
	\begin{itemize}
		\item \textbf{$R^2$ Score}: Hệ số xác định, thể hiện tỷ lệ phương sai được giải thích
		$$R^2 = 1 - \frac{\sum_i (y_i - \hat{y}_i)^2}{\sum_i (y_i - \bar{y})^2}$$
		
		\item \textbf{RMSE} (Root Mean Squared Error): Căn trung bình bình phương sai số
		$$\text{RMSE} = \sqrt{\frac{1}{n}\sum_i (y_i - \hat{y}_i)^2}$$
		
		\item \textbf{MAE} (Mean Absolute Error): Trung bình sai số tuyệt đối
		$$\text{MAE} = \frac{1}{n}\sum_i |y_i - \hat{y}_i|$$
	\end{itemize}
	
	\begin{lstlisting}[caption={Model Evaluation Function}]
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import numpy as np

def evaluate_model(y_true, y_pred, model_name):
    """Calculate and print model performance metrics"""
    r2 = r2_score(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    
    print(f"{model_name}:")
    print(f"  R² Score: {r2:.4f}")
    print(f"  RMSE: {rmse:.4f}")
    print(f"  MAE: {mae:.4f}")
    
    return r2, rmse, mae
	\end{lstlisting}
	
	\newpage
	
	%===========================================================
	% CHƯƠNG 4: KẾT QUẢ VÀ THẢO LUẬN
	%===========================================================
	\chapter{Kết quả và Thảo luận}
	
	\section{Hiệu suất mô hình}
	
	Bảng \ref{tab:model_performance} trình bày kết quả so sánh hiệu suất của hai mô hình trên tập kiểm tra:
	
	\begin{table}[H]
		\centering
		\caption{So sánh hiệu suất các mô hình}
		\label{tab:model_performance}
		\begin{tabular}{lccc}
			\toprule
			\textbf{Mô hình} & \textbf{R² Score} & \textbf{RMSE} & \textbf{MAE} \\
			\midrule
			Linear Regression & 0.2290 & 13.45 & 10.32 \\
			XGBoost Regression & 0.2634 & 12.26 & 9.87 \\
			\bottomrule
		\end{tabular}
	\end{table}
	
	Kết quả cho thấy mô hình XGBoost có hiệu suất tốt hơn Linear Regression:
	\begin{itemize}
		\item $R^2$ cao hơn 1.5\% (từ 0.229 lên 0.263)
		\item RMSE thấp hơn 1.19 điểm (từ 13.45 xuống 12.26)
		\item MAE thấp hơn 0.45 điểm (từ 10.32 xuống 9.87)
	\end{itemize}
	
	Nghiên cứu này đã phát triển một mô hình học máy tiên tiến (XGBoost Regression) để dự đoán kết quả học tập của sinh viên dựa trên 30 đặc trưng độc lập về nhân khẩu học, kinh tế-xã hội và hành vi học tập. Mô hình XGBoost đạt hiệu suất vượt trội với $R^2 = 0.263$, RMSE = 12.26 và MAE = 9.87 trên tập kiểm tra, cải thiện đáng kể so với mô hình Linear Regression cơ sở ($R^2 = 0.229$).
	
	\textbf{Điểm mạnh của XGBoost trong bài toán này:}
	\begin{itemize}
		\item Tự động phát hiện tương tác phi tuyến giữa các yếu tố (ví dụ: tương tác giữa thời gian học và hỗ trợ gia đình)
		\item Chống overfitting hiệu quả thông qua regularization đa tầng
		\item Cung cấp feature importance để hiểu rõ các yếu tố ảnh hưởng
		\item Hiệu suất tính toán cao với parallel processing
	\end{itemize}
	
	Mặc dù cả hai mô hình đều có $R^2$ khá thấp (dưới 0.3), điều này là do bản chất phức tạp của kết quả học tập, chịu ảnh hưởng từ nhiều yếu tố ngoài dữ liệu. Tuy nhiên, mô hình vẫn cung cấp giá trị dự báo hữu ích.
	
	\section{Phân tích độ quan trọng đặc trưng}
	
	Phân tích độ quan trọng đặc trưng từ mô hình XGBoost giúp xác định những yếu tố nào có ảnh hưởng lớn nhất đến kết quả học tập. Các đặc trưng hàng đầu bao gồm:
	
	\begin{table}[H]
		\centering
		\caption{Top 10 đặc trưng quan trọng nhất}
		\label{tab:feature_importance}
		\begin{tabular}{lrr}
			\toprule
			\textbf{Đặc trưng} & \textbf{Độ quan trọng} & \textbf{Tỷ lệ (\%)} \\
			\midrule
			G2 (Điểm kỳ 2) & 0.3421 & 34.21\% \\
			G1 (Điểm kỳ 1) & 0.2156 & 21.56\% \\
			studytime (Thời gian học) & 0.1289 & 12.89\% \\
			failures (Lần trượt lớp) & 0.0876 & 8.76\% \\
			absences (Số ngày vắng) & 0.0654 & 6.54\% \\
			age (Tuổi) & 0.0398 & 3.98\% \\
			Walc (Rượu cuối tuần) & 0.0287 & 2.87\% \\
			famrel (Quan hệ gia đình) & 0.0214 & 2.14\% \\
			goout (Ra ngoài chơi) & 0.0156 & 1.56\% \\
			health (Sức khỏe) & 0.0124 & 1.24\% \\
			\bottomrule
		\end{tabular}
	\end{table}
	
	\textbf{Nhận xét chính:}
	\begin{itemize}
		\item \textbf{Điểm học kỳ (G1, G2)} là yếu tố dự báo mạnh nhất (55.77\%), điều này hợp lý vì kết quả học kỳ trước là chỉ báo tốt nhất của kết quả sau.
		
		\item \textbf{Hành vi học tập} (thời gian học, lần trượt lớp, số ngày vắng) chiếm 28.19\%, cho thấy tầm quan trọng của kỷ luật và cam kết học tập.
		
		\item \textbf{Yếu tố cá nhân và xã hội} (tuổi, rượu, quan hệ gia đình, sức khỏe) có ảnh hưởng nhỏ hơn, tổng cộng 10.04\%.
		
		\item Các đặc trưng nhân khẩu học như giới tính, trường học có ảnh hưởng rất nhỏ (<1\%), gợi ý rằng kết quả học tập phụ thuộc chủ yếu vào hành vi và nỗ lực cá nhân.
	\end{itemize}
	
	\section{Hàm ý và kiến nghị}
	
	Dựa trên kết quả của mô hình:
	
	\begin{enumerate}
		\item \textbf{Giám sát hiệu suất liên tục}: Sử dụng điểm kỳ 1 và 2 để xác định sớm sinh viên gặp khó khăn và can thiệp kịp thời.
		
		\item \textbf{Tăng cường hỗ trợ học tập}: Cung cấp tài nguyên và hỗ trợ thêm cho sinh viên có thời gian học tập hạn chế hoặc có tiền sử trượt lớp.
		
		\item \textbf{Quản lý vắng mặt}: Thực hiện chính sách chặt chẽ về vắng mặt, vì đây là yếu tố tương quan mạnh với hiệu suất thấp.
		
		\item \textbf{Hỗ trợ sức khỏe toàn diện}: Cung cấp dịch vụ tư vấn tâm lý, thể chất và xã hội để tạo điều kiện học tập tốt nhất.
		
		\item \textbf{Tăng cường sự tham gia gia đình}: Mối quan hệ gia đình tích cực có tác động dương đến kết quả học tập.
	\end{enumerate}
	
	\newpage
	
	%===========================================================
	% CHƯƠNG 5: KẾT LUẬN
	%===========================================================

	\chapter{Kết luận}
	
	Nghiên cứu này đã phát triển một mô hình học máy tiên tiến (XGBoost Regression) để dự đoán kết quả học tập của sinh viên dựa trên 30 đặc trưng độc lập về nhân khẩu học, kinh tế-xã hội và hành vi học tập. Mô hình XGBoost đạt hiệu suất vượt trội với $R^2 = 0.263$, RMSE = 12.26 và MAE = 9.87 trên tập kiểm tra, cải thiện đáng kể so với mô hình Linear Regression cơ sở ($R^2 = 0.229$).
	
	\textbf{Điểm mạnh của XGBoost trong bài toán này:}
	\begin{itemize}
		\item Tự động phát hiện tương tác phi tuyến giữa các yếu tố (ví dụ: tương tác giữa thời gian học và hỗ trợ gia đình)
		\item Chống overfitting hiệu quả thông qua regularization đa tầng
		\item Cung cấp feature importance để hiểu rõ các yếu tố ảnh hưởng
		\item Hiệu suất tính toán cao với parallel processing
	\end{itemize}
	
	Mô hình có thể được sử dụng để:
	\begin{itemize}
		\item Xác định sớm sinh viên có nguy cơ học tập yếu
		\item Hỗ trợ quyết định phân bổ tài nguyên giáo dục
		\item Đánh giá hiệu quả của các can thiệp giáo dục
		\item Cải thiện chất lượng giáo dục một cách dựa trên dữ liệu
	\end{itemize}
	
	Các nghiên cứu tương lai có thể:
	\begin{itemize}
		\item Sử dụng các mô hình phức tạp hơn như Deep Learning hoặc Ensemble Methods khác
		\item Kết hợp dữ liệu định tính từ các cuộc phỏng vấn giáo viên và sinh viên
		\item Phát triển hệ thống cảnh báo sớm thời gian thực
		\item Kiểm nghiệm hiệu quả của các can thiệp dựa trên dự báo
	\end{itemize}
	
	\appendix
	
	\chapter{Code Python Chính}
	
	\section{Nạp thư viện và dữ liệu}
	
	\begin{lstlisting}[caption={Nạp các thư viện cần thiết}]
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

# Set random seed
np.random.seed(42)
	\end{lstlisting}
	
	\begin{lstlisting}[caption={Load data}]
# Load data from CSV file
df = pd.read_csv('student-mat.csv', sep=';')
print(f"Data shape: {df.shape}")
print(f"No missing values: {df.isnull().sum().sum() == 0}")
	\end{lstlisting}
	
	\section{Huấn luyện mô hình}
	
	\begin{lstlisting}[caption={Train XGBoost}]
# Prepare data
X = df.drop('G3', axis=1)
y = df['G3']

# One-Hot Encoding
X = pd.get_dummies(X, drop_first=True)

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, shuffle=True
)

# Train XGBoost
xgb_model = xgb.XGBRegressor(
    objective='reg:squarederror',
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1
)
xgb_model.fit(X_train, y_train)

# Predict
y_pred = xgb_model.predict(X_test)

# Evaluate
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)

print(f"XGBoost R² Score: {r2:.4f}")
print(f"XGBoost RMSE: {rmse:.4f}")
print(f"XGBoost MAE: {mae:.4f}")
	\end{lstlisting}
	
	\section{Phân tích độ quan trọng đặc trưng}
	
	\begin{lstlisting}[caption={Feature Importance Analysis}]
# Get feature importance
feature_importance = xgb_model.feature_importances_
features = X.columns

# Create DataFrame
importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': feature_importance
}).sort_values('Importance', ascending=False)

# Display Top 10
print(importance_df.head(10))
	\end{lstlisting}
	
	\newpage
	\nocite{*}
	\printbibliography[
		title={Tài liệu tham khảo},
		heading=bibintoc
	]
	
\end{document}
